{"cells":[{"cell_type":"markdown","id":"8b16ec0d","metadata":{"id":"8b16ec0d"},"source":["# ML Course 2024 |  Medical Expenses Prediction Challenge\n","\n","This notebook should serve as a starting point to work on the project. Please read the project description first."]},{"cell_type":"code","execution_count":null,"id":"3ce7d1bf","metadata":{},"outputs":[],"source":["# Just so that you don't have to restart the notebook with every change.\n","%load_ext autoreload\n","%autoreload 2 "]},{"cell_type":"markdown","id":"f6a3a951","metadata":{"id":"f6a3a951"},"source":["# Set team ID\n","Important: set your Team ID here. You can find it in CMS."]},{"cell_type":"code","execution_count":62,"id":"b2bdc09f","metadata":{"executionInfo":{"elapsed":377,"status":"ok","timestamp":1716808582700,"user":{"displayName":"Jonas Klesen","userId":"16201315161566161330"},"user_tz":-120},"id":"b2bdc09f"},"outputs":[],"source":["team_id = \"18\"  # put your team id here"]},{"cell_type":"markdown","id":"wJs2YvIMvO9Q","metadata":{"id":"wJs2YvIMvO9Q"},"source":["# [Colab only] Connect to your Google Drive"]},{"cell_type":"code","execution_count":63,"id":"SHVae3MHvI9g","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1557,"status":"ok","timestamp":1716808584619,"user":{"displayName":"Jonas Klesen","userId":"16201315161566161330"},"user_tz":-120},"id":"SHVae3MHvI9g","outputId":"c13669b0-1bb0-42ac-bdd5-dde5bba2485c"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":64,"id":"UeTBTeKZvr0z","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1716808584620,"user":{"displayName":"Jonas Klesen","userId":"16201315161566161330"},"user_tz":-120},"id":"UeTBTeKZvr0z","outputId":"56d405ce-0539-4c9f-e15f-1022eb3c5c7e"},"outputs":[],"source":["# %cd \"/content/drive/MyDrive/path/to/your/project\""]},{"cell_type":"markdown","id":"19af9755","metadata":{"id":"19af9755"},"source":["# Imports"]},{"cell_type":"markdown","id":"UJskkHs1wW6r","metadata":{"id":"UJskkHs1wW6r"},"source":["[Colab only] Note: if you need to install any packages, run a code cell with content `!pip install packagename`"]},{"cell_type":"code","execution_count":65,"id":"8e2de5bd","metadata":{},"outputs":[],"source":["from utils import config\n","from utils.config import PAPER_STYLE"]},{"cell_type":"code","execution_count":66,"id":"b0153603","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1716808584620,"user":{"displayName":"Jonas Klesen","userId":"16201315161566161330"},"user_tz":-120},"id":"b0153603"},"outputs":[],"source":["import os\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import sklearn.metrics as skm\n","from prettytable import PrettyTable\n","from scipy import stats\n","from scipy.stats import norm, skew  # for some statistics\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.decomposition import PCA, FastICA\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.feature_selection import RFECV, SelectKBest, VarianceThreshold, chi2, f_classif, mutual_info_classif\n","from sklearn.kernel_approximation import Nystroem\n","from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, SGDClassifier\n","from sklearn.model_selection import (\n","    GridSearchCV,\n","    ShuffleSplit,\n","    StratifiedKFold,\n","    TunedThresholdClassifierCV,\n","    train_test_split,\n",")\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n","from sklearn.preprocessing import (\n","    FunctionTransformer,\n","    LabelEncoder,\n","    MinMaxScaler,\n","    Normalizer,\n","    OneHotEncoder,\n","    PolynomialFeatures,\n","    PowerTransformer,\n","    QuantileTransformer,\n","    RobustScaler,\n","    StandardScaler,\n","    quantile_transform,\n",")\n","from sklearn.svm import SVC, SVR, LinearSVC, LinearSVR, NuSVC, NuSVR\n","from umap import UMAP\n","from xgboost import XGBClassifier\n","\n","from utils import experiments, helpers\n","from utils import metrics as my_metrics\n","from utils import plots, scorers\n","from utils import statistical as st\n","from utils import tuning\n","from utils.param_grids import (\n","    choose_param_grid,\n","    combine_param_grids,\n","    construct_param_grids_list,\n","    make_smaller_param_grid,\n",")\n","from utils.transformers import PolynomialColumnTransformer\n","\n","# Use cuML to use GPU-accelerated models\n","USE_CUML = False\n","\n","# Run experiments\n","RUN_EXPERIMENTS = False\n","EXECUTE_CORRELATION_ANALYSIS = False\n","CORRELATION_THRESHOLD = 0.9\n","\n","# True if no plots should be displayed\n","NO_PLOTS = False\n","\n","if config.CUML_INSTALLED and USE_CUML:\n","    from cuml import LogisticRegression, MBSGDClassifier\n","    from cuml.common.device_selection import get_global_device_type, set_global_device_type\n","    from cuml.svm import SVC, SVR\n","\n","    set_global_device_type(\"gpu\")\n","\n","    print(\"cuML's default execution device:\", get_global_device_type())\n","\n","# Constants that define the classification and regression targets\n","CLF_TARGET = \"UTILIZATION\"\n","REG_TARGET = \"TOT_MED_EXP\"\n","\n","# Dedicate a fraction of the data for testing (validation is taken care of by CV)\n","TEST_SIZE = 0.2\n","\n","# Define a RANDOM_STATE to make outputs deterministic\n","RANDOM_STATE = 42\n","helpers.seed_everything(RANDOM_STATE)"]},{"cell_type":"markdown","id":"1c3a7004","metadata":{},"source":["# **Data Analysis & Preprocessing**"]},{"cell_type":"markdown","id":"2f5eaed7","metadata":{"id":"2f5eaed7"},"source":["## Load Data\n","\n","In a first step, we load the provided training data from the csv file"]},{"cell_type":"code","execution_count":null,"id":"2fc6b0bb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1300,"status":"ok","timestamp":1716808585916,"user":{"displayName":"Jonas Klesen","userId":"16201315161566161330"},"user_tz":-120},"id":"2fc6b0bb","outputId":"98874125-e5d1-463c-9ac9-d8b04976b814"},"outputs":[],"source":["df_train = pd.read_csv(\"data/train.csv\")\n","df_train.drop(columns=[REG_TARGET], inplace=True) # drop the regression target\n","\n","print(\"The loaded dataset has {} rows and {} columns\".format(df_train.shape[0], df_train.shape[1]))"]},{"cell_type":"code","execution_count":null,"id":"c321d106","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"executionInfo":{"elapsed":566,"status":"ok","timestamp":1716808586474,"user":{"displayName":"Jonas Klesen","userId":"16201315161566161330"},"user_tz":-120},"id":"c321d106","outputId":"6628d002-66eb-420d-e59f-0969d59fe13f"},"outputs":[],"source":["df_train.head()"]},{"cell_type":"code","execution_count":null,"id":"59576dd7","metadata":{},"outputs":[],"source":["# Handling missing values\n","total_missing_values = df_train.isnull().sum().sum()\n","print(f\"Total number of missing values: {total_missing_values}\")"]},{"cell_type":"markdown","id":"9c9633f0","metadata":{"id":"9c9633f0"},"source":["## Data exploration"]},{"cell_type":"code","execution_count":null,"id":"73924d31","metadata":{},"outputs":[],"source":["print(\n","    helpers.describe_cols(\n","        df_train,\n","        dummy_is_categorical=False,\n","        consecutive_sequences_are_categorical=False,\n","        low_unique_int_values_are_categorical=False,\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"id":"5869a2b7","metadata":{},"outputs":[],"source":["categorical_cols, numerical_cols = helpers.categorical_and_numerical_columns(\n","    df_train,\n","    consecutive_sequences_are_categorical=False,\n","    low_unique_int_values_are_categorical=False,\n",")\n","\n","# One-Hot encoding for categorical columns (with non-numeric values only)\n","categorical_encoder, df_train = helpers.handle_categorical_cols(\n","    df_train, categorical_cols, return_only_encoded=False\n",")\n","df_train = df_train.copy()\n","\n","print(f\"Numerical ({len(numerical_cols)}) \", numerical_cols)\n","print(f\"Categorical ({len(categorical_cols)}): \", categorical_cols)\n","print(f\"Number of columns after one-hot encoding: {df_train.shape[1]}\")"]},{"cell_type":"code","execution_count":null,"id":"8ae83ef6","metadata":{},"outputs":[],"source":["# RACE_White\n","# If 1.0, the person is white, otherwise not\n","# UTILIZATION_LOW\n","# If 1.0, the person has low utilization, otherwise high\n","df_train.rename(columns={\"RACE_White\": \"RACE\"}, inplace=True)\n","df_train.rename(columns={f\"{CLF_TARGET}_LOW\": f\"{CLF_TARGET}\"}, inplace=True)\n","\n","CLASS_MAPPING = {1: \"LOW\", 0: \"HIGH\"}\n","\n","print(\n","    helpers.describe_cols(\n","        df_train,\n","        dummy_is_categorical=True,\n","        consecutive_sequences_are_categorical=False,\n","        low_unique_int_values_are_categorical=False,\n","    )\n",")"]},{"cell_type":"markdown","id":"f77c8b59","metadata":{},"source":["### *Correlation Analysis*"]},{"cell_type":"code","execution_count":73,"id":"96ea92a2","metadata":{},"outputs":[],"source":["correlated_cols = []\n","if EXECUTE_CORRELATION_ANALYSIS:\n","    # Get the correlation matrix of the data (excluding the regression and classification targets!)\n","    # Find the columns that are highly correlated with each other and remove them\n","    CORRELATION_THRESHOLD = 0.9  # 90% correlation threshold\n","\n","    df_train_without_clf_target = df_train.drop(columns=[CLF_TARGET])\n","    correlated_cols, corr, summary = st.correlated_columns(\n","        df_train_without_clf_target, threshold=CORRELATION_THRESHOLD\n","    )\n","\n","    print(f\"\\nFound {len(correlated_cols)} cols with correlation >= {CORRELATION_THRESHOLD}\")\n","    print(correlated_cols)\n","\n","    if not NO_PLOTS:\n","        with plt.style.context(PAPER_STYLE):\n","            plt.figure(figsize=(10, 8), dpi=600)  # Increased figure size\n","            sns.heatmap(corr, cmap=plt.cm.CMRmap_r, annot=False, xticklabels=False, yticklabels=False)\n","            plt.savefig(\"correlation_heatmap.pdf\", bbox_inches=\"tight\", format=\"pdf\", dpi=600)\n","            plt.tight_layout()\n","            plt.show()\n","\n","    summary_table = helpers.make_pretty_table(\n","        summary,\n","        [\"Correlated Column\", \"Correlated With\", \"Correlation\", \"p-value\"],\n","        title=\"Correlation Summary\",\n","    )\n","    print(summary_table)\n","\n","    print(\n","        f\"Number of columns after dropping correlated columns: {df_train.shape[1] - len(correlated_cols)}\"\n","    )"]},{"cell_type":"markdown","id":"d4dcdd84","metadata":{},"source":["### *One-Hot Encoding of Likely Categorical Features*\n"]},{"cell_type":"code","execution_count":null,"id":"cbda0d2a","metadata":{},"outputs":[],"source":["categorical_cols, numerical_cols = helpers.categorical_and_numerical_columns(\n","    # Drop the target variable and other columns that will be dropped during training\n","    df_train.drop(columns=[CLF_TARGET] + correlated_cols), \n","    dummy_is_categorical=True,\n","    consecutive_sequences_are_categorical=True,\n","    low_unique_int_values_are_categorical=True,\n",")\n","\n","categorical_encoder, df_train = helpers.handle_categorical_cols(\n","    df_train, categorical_cols, return_only_encoded=False\n",")\n","\n","print(\n","    helpers.describe_cols(\n","        df_train,\n","        dummy_is_categorical=True,\n","        consecutive_sequences_are_categorical=True,\n","        low_unique_int_values_are_categorical=True,\n","    )\n",")\n","\n","print(f\"Number of numerical columns: {len(numerical_cols)}\")\n","print(f\"Number of categorical columns: {len(categorical_cols)}\")"]},{"cell_type":"markdown","id":"8cd7d21a","metadata":{},"source":["### *Remove Correlated Features (again)*"]},{"cell_type":"code","execution_count":75,"id":"46b29c3e","metadata":{},"outputs":[],"source":["if EXECUTE_CORRELATION_ANALYSIS:\n","    # Get the correlation matrix of the data (excluding the regression and classification targets!)\n","    # Find the columns that are highly correlated with each other and remove them\n","    CORRELATION_THRESHOLD = 0.9  # 90% correlation threshold\n","\n","    df_train_without_clf_target = df_train.drop(columns=[CLF_TARGET] + correlated_cols)\n","    new_correlated_cols, corr, summary = st.correlated_columns(\n","        df_train_without_clf_target, threshold=CORRELATION_THRESHOLD\n","    )\n","\n","    print(f\"\\nFound {len(new_correlated_cols)} cols with correlation >= {CORRELATION_THRESHOLD}\")\n","    print(new_correlated_cols)\n","\n","    if not NO_PLOTS:\n","        with plt.style.context(PAPER_STYLE):\n","            plt.figure(figsize=(10, 8), dpi=600)  # Increased figure size\n","            sns.heatmap(corr, cmap=plt.cm.CMRmap_r, annot=False, xticklabels=False, yticklabels=False)\n","            plt.savefig(\"correlation_heatmap_after.pdf\", bbox_inches=\"tight\", format=\"pdf\", dpi=600)\n","            plt.tight_layout()\n","            plt.show()\n","\n","    summary_table = helpers.make_pretty_table(\n","        summary,\n","        [\"Correlated Column\", \"Correlated With\", \"Correlation\", \"p-value\"],\n","        title=\"Correlation Summary\",\n","    )\n","    print(summary_table)\n","\n","    correlated_cols += new_correlated_cols\n","    print(\n","        f\"Number of columns after dropping correlated columns: {df_train.shape[1] - len(new_correlated_cols)}\"\n","    )"]},{"cell_type":"code","execution_count":76,"id":"ab0a4f16","metadata":{},"outputs":[],"source":["# After Feature Selection\n","if EXECUTE_CORRELATION_ANALYSIS:\n","    df_train_without_clf_target = df_train.drop(columns=[CLF_TARGET] + correlated_cols)\n","    _, new_corr, _ = st.correlated_columns(\n","        df_train_without_clf_target, threshold=CORRELATION_THRESHOLD\n","    )\n","\n","    if not NO_PLOTS:\n","        plt.figure(figsize=(6, 4))\n","        sns.heatmap(new_corr, cmap=plt.cm.CMRmap_r)\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"id":"e4639c7c","metadata":{},"outputs":[],"source":["if not NO_PLOTS:\n","    plots.plot_features_vs_target(\n","        df_train,\n","        CLF_TARGET,\n","        figsize=(6, 4),\n","        features=numerical_cols,\n","        style=PAPER_STYLE,\n","        n_features=6,\n","        max_rows=2,\n","        is_categorical=True,\n","        categorical_legend=CLASS_MAPPING,\n","        x_log_scale=False,\n","    )"]},{"cell_type":"code","execution_count":null,"id":"80679712","metadata":{},"outputs":[],"source":["# Count the number of positive and negative samples\n","pos_samples = (df_train[CLF_TARGET] == 1).sum()\n","neg_samples = (df_train[CLF_TARGET] == 0).sum()\n","\n","if not NO_PLOTS:\n","    plots.visualize(\n","        df_train.drop(columns=[CLF_TARGET] + correlated_cols),\n","        n_components=2,\n","        method=\"ica\",\n","        indices=[df_train[CLF_TARGET] == 0, df_train[CLF_TARGET] == 1],\n","        labels=[\n","            f\"{CLASS_MAPPING[0]}, {neg_samples} samples\",\n","            f\"{CLASS_MAPPING[1]}, {pos_samples} samples\",\n","        ],\n","        style=PAPER_STYLE,\n","    )"]},{"cell_type":"markdown","id":"34410538","metadata":{"id":"34410538"},"source":["# **Linear classification**"]},{"cell_type":"markdown","id":"WeqFeth6hb6w","metadata":{"id":"WeqFeth6hb6w"},"source":["In this part, we will train a simple linear classification model to predict our target `UTILIZATION`.\n"]},{"cell_type":"markdown","id":"_QIyfxXthmTT","metadata":{"id":"_QIyfxXthmTT"},"source":["We will first change our targets (classes: LOW, HIGH) to numeric targets. Then, we solve a logistic regression problem by minimizing the binary cross-entropy function\n","\n","$$\n","J(\\theta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\log(p_{\\theta}(\\hat{y}=1 | \\mathbf{x}_i)) + (1 - y_i) \\log(p_{\\theta}(\\hat{y}=0 | \\mathbf{x}_i)) \\right)\n","$$\n","\n","where $y_i \\in \\{0, 1\\}$ and $p_{\\theta}(\\hat{y}=k | \\mathbf{x}_i)$ is the probability assigned by our model to class $k$ having observed features $\\mathbf{x}_i$.\n","\n","0 refers to HIGH, and 1 refers to LOW"]},{"cell_type":"markdown","id":"09e547a8","metadata":{},"source":["### Setup"]},{"cell_type":"code","execution_count":79,"id":"2a427f49","metadata":{},"outputs":[],"source":["RUNS_DIR = \"classification_task_runs\"\n","\n","helpers.ensure_directory_exists(RUNS_DIR)"]},{"cell_type":"code","execution_count":null,"id":"16f802f4","metadata":{},"outputs":[],"source":["# Split into features and target for classification\n","X = df_train.drop(columns=[CLF_TARGET])\n","y = df_train[CLF_TARGET]\n","\n","print(f\"Class mapping of column {CLF_TARGET}: {CLASS_MAPPING}\\n\")\n","value_counts_dict = y.value_counts().to_dict()\n","\n","print(f\"Shape of X: {X.shape}\")\n","print(f\"Shape of y: {y.shape}\", f\", value counts in y: {value_counts_dict}\")"]},{"cell_type":"markdown","id":"f6d08d2d","metadata":{"id":"f6d08d2d"},"source":["### Process the data"]},{"cell_type":"code","execution_count":null,"id":"5f516bc9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":744,"status":"ok","timestamp":1716809033654,"user":{"displayName":"Jonas Klesen","userId":"16201315161566161330"},"user_tz":-120},"id":"5f516bc9","outputId":"277dbcdf-5798-4312-a723-3d1ae52645c6"},"outputs":[],"source":["X_train, X_test, y_train, y_test = helpers.make_train_test_split(\n","    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",")"]},{"cell_type":"markdown","id":"0S5pf_bA2XAV","metadata":{"id":"0S5pf_bA2XAV"},"source":["### Fit a model by using training data"]},{"cell_type":"code","execution_count":82,"id":"32aad7a4","metadata":{},"outputs":[],"source":["if RUN_EXPERIMENTS:\n","    CLASS_MAPPING = {1: \"LOW\", 0: \"HIGH\"}\n","\n","    # Remove Correlated + Encode (+Likely Categorical)\n","    # dummy_is_categorical=True,\n","    # encode_after_remove_correlated=True,\n","    # consecutive_sequences_are_categorical=True,\n","    # low_unique_int_values_are_categorical=True,\n","\n","    # Encode (+Likely Categorical) + Remove Correlated\n","    # dummy_is_categorical=True,\n","    # encode_after_remove_correlated=False,\n","    # consecutive_sequences_are_categorical=True,\n","    # low_unique_int_values_are_categorical=True,\n","\n","    # Encode + Remove Correlated\n","    # dummy_is_categorical=True,\n","    # encode_after_remove_correlated=False,\n","    # consecutive_sequences_are_categorical=False,\n","    # low_unique_int_values_are_categorical=False,\n","\n","    df_train, categorical_encoder, categorical_cols, numerical_cols, correlated_cols, data = (\n","        experiments.prepare_df_train_pipeline(\n","            \"data/train.csv\",\n","            CLF_TARGET,\n","            target_is_categorical=True,\n","            target_categorical_mapping=CLASS_MAPPING,\n","            remove_cols=[REG_TARGET],\n","            remove_correlated=False,\n","            correlation_threshold=CORRELATION_THRESHOLD,\n","            dummy_is_categorical=True,\n","            encode_after_remove_correlated=False,\n","            consecutive_sequences_are_categorical=True,\n","            low_unique_int_values_are_categorical=True,\n","            test_size=TEST_SIZE,\n","            random_state=RANDOM_STATE,\n","            stratify=True,\n","            tabs=1,\n","        )\n","    )\n","\n","    df_train_desc = helpers.describe_cols(\n","        df_train,\n","        tabs=1,\n","        dummy_is_categorical=True,\n","        consecutive_sequences_are_categorical=True,\n","        low_unique_int_values_are_categorical=True,\n","    )\n","\n","    X_train, y_train, X_test, y_test = data\n","\n","    pipeline = experiments.preprocessing_pipeline2(\n","        drop_columns=correlated_cols, numerical_scaler=StandardScaler()\n","    )\n","    pipeline = experiments.extend_pipeline(\n","        pipeline,\n","        (\"classifier\", None),\n","    )\n","\n","    # Define the parameter grid\n","    base_param_grid = {\n","        # Classifiers\n","        \"classifier\": [\n","            LogisticRegression(),\n","        ],\n","    }\n","\n","    # Make a list of param grids by combining the preprocessing and classifier hyperparameters\n","    param_grids = construct_param_grids_list(base_param_grid, key=\"classifier\")\n","\n","    # Iterate over each classifier\n","    tuning.run_model_search(\n","        X_train,\n","        y_train,\n","        X_test,\n","        y_test,\n","        param_grids,\n","        pipeline,\n","        task_type=\"classification\",\n","        refit_classification=\"f1-score\",\n","        output_dir=RUNS_DIR,\n","        run_dir_suffix=\"Encode (+Likely Categorical)\",\n","        additional_save_on_run={\n","            \"categorical_encoder\": (categorical_encoder, \"pkl\"),\n","            \"categorical_cols\": (categorical_cols, \"pkl\"), \n","            \"numerical_cols\": (numerical_cols, \"pkl\"),\n","            \"correlated_cols\": (correlated_cols, \"txt\"),\n","            \"df_train_desc\": (df_train_desc, \"txt\"),\n","        },\n","        use_cuml=USE_CUML,\n","        subset=1,\n","        tabs=1,\n","        verbose=10,\n","    )"]},{"cell_type":"code","execution_count":null,"id":"57d8633e","metadata":{},"outputs":[],"source":["summary_df = tuning.plot_runs(\n","    RUNS_DIR,\n","    [\"f1-score\"],\n","    only_runs_with_this_in_name=[\"LogisticRegression\"],\n","    map_x_axis_with_str=\"E\",\n","    summary_table=True,\n","    figsize=(6, 3),\n","    title=None,\n","    style=PAPER_STYLE,\n","    save_path=os.path.join(RUNS_DIR, \"encode_correlation_performance_comparison\"),\n","    save_format=\"pdf\",\n","    xtick_rotation=0,\n","    xtick_size=8,\n","    annotate_values=True,\n","    dpi=600,\n","    grid=True,\n","    use_adjust_text=True,\n","    y_max=0.79\n",")"]},{"cell_type":"markdown","id":"f527af99","metadata":{},"source":["### Test Dimensionality Reduction Techniques"]},{"cell_type":"code","execution_count":null,"id":"2e433478","metadata":{},"outputs":[],"source":["classifier_to_test = \"run_20240822_132045_LogisticRegression_Remove Correlated + Encode (+Likely Categorical)\"\n","classifier_to_test_name = tuning.load_run_results(os.path.join(RUNS_DIR, classifier_to_test))[\"name\"]\n","best_params = tuning.load_from_run(os.path.join(RUNS_DIR, classifier_to_test), \"best_params.pkl\")\n","\n","# Preprocess the best_params so that it can be passed to the model(**best_params)\n","best_params_copy = best_params.copy()\n","for key in best_params_copy.keys():\n","    if key.startswith(\"classifier__\"):\n","        best_params[key.split(\"__\")[1]] = best_params.pop(key)\n","\n","if \"classifier\" in best_params:\n","    del best_params[\"classifier\"]\n","\n","print(f\"Best params for {classifier_to_test_name}:\", best_params)"]},{"cell_type":"code","execution_count":85,"id":"5bbed8da","metadata":{},"outputs":[],"source":["if RUN_EXPERIMENTS:\n","    CLASS_MAPPING = {1: \"LOW\", 0: \"HIGH\"}\n","    dimensionality_reduction_methods = [\n","        # FunctionTransformer(),\n","        # PCA(random_state=RANDOM_STATE),\n","        # LinearDiscriminantAnalysis(),\n","        # FastICA(random_state=RANDOM_STATE),\n","        UMAP(random_state=RANDOM_STATE),\n","    ]\n","\n","    for dimensionality_reduction_method in dimensionality_reduction_methods:\n","        df_train, categorical_encoder, categorical_cols, numerical_cols, correlated_cols, data = (\n","            experiments.prepare_df_train_pipeline(\n","                \"data/train.csv\",\n","                CLF_TARGET,\n","                target_is_categorical=True,\n","                target_categorical_mapping=CLASS_MAPPING,\n","                remove_cols=[REG_TARGET],\n","                remove_correlated=True,\n","                correlation_threshold=CORRELATION_THRESHOLD,\n","                dummy_is_categorical=True,\n","                encode_after_remove_correlated=True,\n","                consecutive_sequences_are_categorical=True,\n","                low_unique_int_values_are_categorical=True,\n","                test_size=TEST_SIZE,\n","                random_state=RANDOM_STATE,\n","                stratify=True,\n","                tabs=1,\n","            )\n","        )\n","\n","        df_train_desc = helpers.describe_cols(\n","            df_train,\n","            tabs=1,\n","            dummy_is_categorical=True,\n","            consecutive_sequences_are_categorical=False,\n","            low_unique_int_values_are_categorical=False,\n","        )\n","\n","        X_train, y_train, X_test, y_test = data\n","\n","        pipeline = experiments.preprocessing_pipeline2(drop_columns=correlated_cols, numerical_scaler=StandardScaler())\n","        pipeline = experiments.extend_pipeline(\n","            pipeline,\n","            (\"dimensionality_reduction\", dimensionality_reduction_method),\n","            (\"classifier\", LogisticRegression(**best_params)),\n","        )\n","\n","        # Define the parameter grid\n","        if not isinstance(dimensionality_reduction_method, FunctionTransformer):\n","            param_grid = {\n","                \"dimensionality_reduction__n_components\": [2, 10, 50, 100, X_train.shape[1]],\n","                \"classifier\": [LogisticRegression(**best_params)],\n","            }\n","        else:\n","            param_grid = {\"classifier\": [LogisticRegression(**best_params)]}\n","\n","        # For LinearDiscriminantAnalysis, we need to set the number of components to the number of classes - 1\n","        if isinstance(dimensionality_reduction_method, LinearDiscriminantAnalysis):\n","            param_grid[\"dimensionality_reduction__n_components\"] = [len(CLASS_MAPPING) - 1]\n","            param_grid[\"dimensionality_reduction__solver\"] = [\"svd\", \"lsqr\", \"eigen\"]\n","\n","        if isinstance(dimensionality_reduction_method, UMAP):\n","            param_grid[\"dimensionality_reduction__metric\"] = [\"euclidean\", \"manhattan\", \"cosine\"]\n","        \n","        if isinstance(dimensionality_reduction_method, FastICA):\n","            param_grid[\"dimensionality_reduction__whiten\"] = [\"arbitrary-variance\", \"unit-variance\", False]\n","            param_grid[\"dimensionality_reduction__whiten_solver\"] = [\"eigh\", \"svd\"]\n","\n","        # Iterate over each classifier\n","        tuning.run_model_search(\n","            X_train,\n","            y_train,\n","            X_test,\n","            y_test,\n","            param_grid,\n","            pipeline,\n","            task_type=\"classification\",\n","            refit_classification=\"f1-score\",\n","            output_dir=os.path.join(RUNS_DIR, \"dimensionality_reduction\"),\n","            run_dir_suffix=dimensionality_reduction_method.__class__.__name__,\n","            additional_save_on_run={\n","                \"categorical_encoder\": (categorical_encoder, \"pkl\"),\n","                \"categorical_cols\": (categorical_cols, \"pkl\"),\n","                \"numerical_cols\": (numerical_cols, \"pkl\"),\n","                \"correlated_cols\": (correlated_cols, \"txt\"),\n","                \"df_train_desc\": (df_train_desc, \"txt\"),\n","            },\n","            use_cuml=USE_CUML,\n","            tabs=1,\n","            verbose=10,\n","        )"]},{"cell_type":"code","execution_count":null,"id":"63f53367","metadata":{},"outputs":[],"source":["summary_df = tuning.plot_runs(\n","    os.path.join(RUNS_DIR, \"dimensionality_reduction\"),\n","    [\"f1-score\"],\n","    runs_names_mapping={\n","        \"FunctionTransformer\": \"Baseline\",\n","        \"PCA\": \"PCA\",\n","        \"LinearDiscriminantAnalysis\": \"LDA\",\n","        \"FastICA\": \"ICA\",\n","        \"UMAP\": \"UMAP\",\n","    },\n","    summary_table=True,\n","    figsize=(6, 3),\n","    style=PAPER_STYLE,\n","    title=None,\n","    save_path=os.path.join(RUNS_DIR, \"dimensionality_reduction\", \"performance_comparison\"),\n","    save_format=\"pdf\",\n","    dpi=600,\n","    xtick_rotation=0,\n","    xtick_size=6,\n","    annotate_values=True,\n","    grid=True,\n","    use_adjust_text=True,\n","    y_max=1\n",")"]},{"cell_type":"code","execution_count":null,"id":"4ce58394","metadata":{},"outputs":[],"source":["RUN_EXPERIMENTS = True\n","if RUN_EXPERIMENTS:\n","    CLASS_MAPPING = {1: \"LOW\", 0: \"HIGH\"}\n","    skew_transformer_methods = [\n","        FunctionTransformer(),\n","        PowerTransformer(method=\"yeo-johnson\"),\n","        QuantileTransformer(output_distribution=\"normal\"),\n","        QuantileTransformer(output_distribution=\"uniform\"),\n","        Normalizer(),\n","    ]\n","\n","    for skew_transformer_method in skew_transformer_methods:\n","        df_train, categorical_encoder, categorical_cols, numerical_cols, correlated_cols, data = (\n","            experiments.prepare_df_train_pipeline(\n","                \"data/train.csv\",\n","                CLF_TARGET,\n","                target_is_categorical=True,\n","                target_categorical_mapping=CLASS_MAPPING,\n","                remove_cols=[REG_TARGET],\n","                remove_correlated=True,\n","                correlation_threshold=CORRELATION_THRESHOLD,\n","                dummy_is_categorical=True,\n","                encode_after_remove_correlated=True,\n","                consecutive_sequences_are_categorical=True,\n","                low_unique_int_values_are_categorical=True,\n","                test_size=TEST_SIZE,\n","                random_state=RANDOM_STATE,\n","                stratify=True,\n","                tabs=1,\n","            )\n","        )\n","\n","        df_train_desc = helpers.describe_cols(\n","            df_train,\n","            tabs=1,\n","            dummy_is_categorical=True,\n","            consecutive_sequences_are_categorical=True,\n","            low_unique_int_values_are_categorical=True,\n","        )\n","\n","        X_train, y_train, X_test, y_test = data\n","\n","        pipeline = experiments.preprocessing_pipeline2(drop_columns=correlated_cols, skew_transformer=skew_transformer_method)\n","        pipeline = experiments.extend_pipeline(\n","            pipeline,\n","            (\"classifier\", LogisticRegression(**best_params)),\n","        )\n","\n","        # Define the parameter grid\n","        param_grid = {\"classifier\": [LogisticRegression(**best_params)]}\n","\n","        run_dir_suffix = skew_transformer_method.__class__.__name__\n","\n","        if isinstance(skew_transformer_method, QuantileTransformer):\n","            run_dir_suffix += f\"_{skew_transformer_method.output_distribution}\"\n","\n","        # Iterate over each classifier\n","        tuning.run_model_search(\n","            X_train,\n","            y_train,\n","            X_test,\n","            y_test,\n","            param_grid,\n","            pipeline,\n","            task_type=\"classification\",\n","            refit_classification=\"f1-score\",\n","            output_dir=os.path.join(RUNS_DIR, \"skew_transformers\"),\n","            run_dir_suffix=run_dir_suffix,\n","            additional_save_on_run={\n","                \"categorical_encoder\": (categorical_encoder, \"pkl\"),\n","                \"categorical_cols\": (categorical_cols, \"pkl\"),\n","                \"numerical_cols\": (numerical_cols, \"pkl\"),\n","                \"correlated_cols\": (correlated_cols, \"txt\"),\n","                \"df_train_desc\": (df_train_desc, \"txt\"),\n","            },\n","            use_cuml=USE_CUML,\n","            tabs=1,\n","            verbose=10,\n","        )"]},{"cell_type":"code","execution_count":null,"id":"b7476f5d","metadata":{},"outputs":[],"source":["summary_df = tuning.plot_runs(\n","    os.path.join(RUNS_DIR, \"skew_transformers\"),\n","    [\"f1-score\"],\n","    runs_names_mapping={\n","        \"FunctionTransformer\": \"Baseline\",\n","        \"PowerTransformer\": \"Yeo-Johnson\",\n","        \"QuantileTransformer_normal\": \"Quantile (Normal)\",\n","        \"QuantileTransformer_uniform\": \"Quantile (Uniform)\",\n","        \"Normalizer\": \"Normalized\",\n","    },\n","    summary_table=True,\n","    figsize=(6, 3),\n","    style=PAPER_STYLE,\n","    title=None,\n","    save_path=os.path.join(RUNS_DIR, \"skew_transformers\", \"performance_comparison\"),\n","    save_format=\"pdf\",\n","    dpi=600,\n","    xtick_rotation=0,\n","    xtick_size=6,\n","    annotate_values=True,\n","    grid=True,\n","    use_adjust_text=True,\n","    y_max=0.79\n",")"]},{"cell_type":"code","execution_count":null,"id":"1b723559","metadata":{},"outputs":[],"source":["RUN_EXPERIMENTS = True\n","if RUN_EXPERIMENTS:\n","    CLASS_MAPPING = {1: \"LOW\", 0: \"HIGH\"}\n","    scalers = [\n","        FunctionTransformer(),\n","        StandardScaler(),\n","        RobustScaler(),\n","        MinMaxScaler(),\n","    ]\n","\n","    for scaler in scalers:\n","        df_train, categorical_encoder, categorical_cols, numerical_cols, correlated_cols, data = (\n","            experiments.prepare_df_train_pipeline(\n","                \"data/train.csv\",\n","                CLF_TARGET,\n","                target_is_categorical=True,\n","                target_categorical_mapping=CLASS_MAPPING,\n","                remove_cols=[REG_TARGET],\n","                remove_correlated=True,\n","                correlation_threshold=CORRELATION_THRESHOLD,\n","                dummy_is_categorical=True,\n","                encode_after_remove_correlated=True,\n","                consecutive_sequences_are_categorical=True,\n","                low_unique_int_values_are_categorical=True,\n","                test_size=TEST_SIZE,\n","                random_state=RANDOM_STATE,\n","                stratify=True,\n","                tabs=1,\n","            )\n","        )\n","\n","        df_train_desc = helpers.describe_cols(\n","            df_train,\n","            tabs=1,\n","            dummy_is_categorical=True,\n","            consecutive_sequences_are_categorical=True,\n","            low_unique_int_values_are_categorical=True,\n","        )\n","\n","        X_train, y_train, X_test, y_test = data\n","\n","        pipeline = experiments.preprocessing_pipeline2(drop_columns=correlated_cols, numerical_scaler=scaler)\n","        pipeline = experiments.extend_pipeline(\n","            pipeline,\n","            (\"classifier\", LogisticRegression(**best_params)),\n","        )\n","\n","        # Define the parameter grid\n","        param_grid = {\"classifier\": [LogisticRegression(**best_params)]}\n","\n","        run_dir_suffix = scaler.__class__.__name__\n","\n","        # Iterate over each classifier\n","        tuning.run_model_search(\n","            X_train,\n","            y_train,\n","            X_test,\n","            y_test,\n","            param_grid,\n","            pipeline,\n","            task_type=\"classification\",\n","            refit_classification=\"f1-score\",\n","            output_dir=os.path.join(RUNS_DIR, \"scalers\"),\n","            run_dir_suffix=run_dir_suffix,\n","            additional_save_on_run={\n","                \"categorical_encoder\": (categorical_encoder, \"pkl\"),\n","                \"categorical_cols\": (categorical_cols, \"pkl\"),\n","                \"numerical_cols\": (numerical_cols, \"pkl\"),\n","                \"correlated_cols\": (correlated_cols, \"txt\"),\n","                \"df_train_desc\": (df_train_desc, \"txt\"),\n","            },\n","            use_cuml=USE_CUML,\n","            tabs=1,\n","            verbose=10,\n","        )"]},{"cell_type":"code","execution_count":null,"id":"04f77b9f","metadata":{},"outputs":[],"source":["summary_df = tuning.plot_runs(\n","    os.path.join(RUNS_DIR, \"scalers\"),\n","    [\"f1-score\"],\n","    runs_names_mapping={\n","        \"FunctionTransformer\": \"Baseline\",\n","        \"StandardScaler\": \"StandardScaler\",\n","        \"RobustScaler\": \"RobustScaler\",\n","        \"MinMaxScaler\": \"MinMaxScaler\"\n","    },\n","    summary_table=True,\n","    figsize=(6, 3),\n","    style=PAPER_STYLE,\n","    title=None,\n","    save_path=os.path.join(RUNS_DIR, \"scalers\", \"performance_comparison\"),\n","    save_format=\"pdf\",\n","    dpi=600,\n","    xtick_rotation=0,\n","    xtick_size=6,\n","    annotate_values=True,\n","    grid=True,\n","    legend_loc=\"upper right\",\n","    use_adjust_text=True,\n","    y_max=0.79\n",")"]},{"cell_type":"code","execution_count":null,"id":"2cca54ea","metadata":{},"outputs":[],"source":["df_train, categorical_encoder, categorical_cols, numerical_cols, correlated_cols, data = (\n","    experiments.prepare_df_train_pipeline(\n","        \"data/train.csv\",\n","        CLF_TARGET,\n","        target_is_categorical=True,\n","        target_categorical_mapping=CLASS_MAPPING,\n","        remove_cols=[REG_TARGET],\n","        remove_correlated=True,\n","        correlation_threshold=CORRELATION_THRESHOLD,\n","        dummy_is_categorical=True,\n","        encode_after_remove_correlated=True,\n","        consecutive_sequences_are_categorical=True,\n","        low_unique_int_values_are_categorical=True,\n","        test_size=TEST_SIZE,\n","        random_state=RANDOM_STATE,\n","        stratify=True,\n","        tabs=1,\n","    )\n",")\n","\n","pipeline = experiments.preprocessing_pipeline2(drop_columns=correlated_cols, numerical_scaler=RobustScaler())\n","\n","X_train, y_train, X_test, y_test = data\n","\n","X_train = pipeline.fit_transform(X_train, y_train)\n","X_test = pipeline.transform(X_test)\n","\n","cv = StratifiedKFold(5)\n","\n","best_params = tuning.load_from_run(\n","    os.path.join(RUNS_DIR, \"final_classifiers_with_select_k_best\", \"run_20240823_205939_GradientBoostingClassifier_\"),\n","    \"best_params.pkl\",\n",")\n","# Preprocess the best_params so that it can be passed to the model(**best_params)\n","best_params_copy = best_params.copy()\n","for key in best_params_copy.keys():\n","    if key.startswith(\"classifier__\"):\n","        best_params[key.split(\"__\")[1]] = best_params.pop(key)\n","\n","if \"classifier\" in best_params:\n","    del best_params[\"classifier\"]\n","if \"select__k\" in best_params:\n","    del best_params[\"select__k\"]\n","    del best_params[\"select__score_func\"]\n","\n","print(f\"Best params:\", best_params)\n","\n","clf = GradientBoostingClassifier(**best_params)\n","rfecv = RFECV(estimator=clf, step=50, cv=cv, scoring=\"f1_macro\", min_features_to_select=1, n_jobs=-2)\n","\n","rfecv.fit(X_train, y_train)\n","\n","print(f\"Optimal number of features: {rfecv.n_features_}\")"]},{"cell_type":"code","execution_count":null,"id":"c3dcab94","metadata":{},"outputs":[],"source":["cv_results = pd.DataFrame(rfecv.cv_results_)\n","\n","with plt.style.context(PAPER_STYLE):\n","    plt.figure(figsize=(6, 4), dpi=600)\n","\n","    # Plot error bars\n","    plt.errorbar(\n","        x=cv_results[\"n_features\"],\n","        y=cv_results[\"mean_test_score\"],\n","        yerr=cv_results[\"std_test_score\"],\n","        fmt=\"o-\",  # Circle markers connected by lines\n","        color=\"black\",\n","        ecolor=\"black\",\n","        elinewidth=0.5,\n","        capsize=2.5,\n","        capthick=0.5,\n","        markersize=4,\n","        linewidth=1,\n","    )\n","\n","    # Set labels and title\n","    plt.xlabel(\"Number of features selected\", fontweight=\"bold\")\n","    plt.ylabel(\"Mean Validation F1-score\", fontweight=\"bold\")\n","\n","    # Set tick parameters\n","    plt.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n","\n","    plt.savefig(os.path.join(RUNS_DIR, \"feature_selection_performance.pdf\"), bbox_inches=\"tight\", format=\"pdf\", dpi=600)\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"f020815b","metadata":{},"outputs":[],"source":["RUN_EXPERIMENTS = True\n","if RUN_EXPERIMENTS:\n","    CLASS_MAPPING = {1: \"LOW\", 0: \"HIGH\"}\n","\n","    df_train, categorical_encoder, categorical_cols, numerical_cols, correlated_cols, data = (\n","        experiments.prepare_df_train_pipeline(\n","            \"data/train.csv\",\n","            CLF_TARGET,\n","            target_is_categorical=True,\n","            target_categorical_mapping=CLASS_MAPPING,\n","            remove_cols=[REG_TARGET],\n","            remove_correlated=True,\n","            correlation_threshold=CORRELATION_THRESHOLD,\n","            dummy_is_categorical=True,\n","            encode_after_remove_correlated=True,\n","            consecutive_sequences_are_categorical=True,\n","            low_unique_int_values_are_categorical=True,\n","            test_size=TEST_SIZE,\n","            random_state=RANDOM_STATE,\n","            stratify=True,\n","            tabs=1,\n","        )\n","    )\n","\n","    pipeline = experiments.preprocessing_pipeline2(drop_columns=correlated_cols, numerical_scaler=RobustScaler())\n","\n","    pipeline = experiments.extend_pipeline(\n","        pipeline,\n","        (\"select\", SelectKBest()),\n","        (\"nystroem\", FunctionTransformer()),  # For kernel approximation (LinearSVC, NuSVC)\n","        (\"classifier\", None),\n","    )\n","\n","    df_train_desc = helpers.describe_cols(\n","        df_train,\n","        tabs=1,\n","        dummy_is_categorical=True,\n","        consecutive_sequences_are_categorical=True,\n","        low_unique_int_values_are_categorical=True,\n","    )\n","    \n","    # Define the parameter grid\n","    base_param_grid = {\n","        \"select__k\": [70],\n","        \"select__score_func\": [mutual_info_classif],\n","        # Classifiers\n","        \"classifier\": [\n","#            GaussianNB(),\n","            LogisticRegression(),\n","            # KNeighborsClassifier(),\n","            # LinearDiscriminantAnalysis(),\n","            # QuadraticDiscriminantAnalysis(),\n","            # RidgeClassifier(),\n","            # PassiveAggressiveClassifier(),\n","            # SGDClassifier(),\n","            # GradientBoostingClassifier(),\n","            # XGBClassifier(),\n","            # LinearSVC(),\n","            # NuSVC(),\n","        ],\n","        \"nystroem\": [Nystroem()],\n","        \"nystroem__n_components\": [100, 500, 1000],\n","        \"nystroem__kernel\": ['rbf', 'poly', 'sigmoid'],\n","    }\n","\n","    # Make a list of param grids by combining the preprocessing and classifier hyperparameters\n","    param_grids = construct_param_grids_list(base_param_grid, key=\"classifier\")\n","\n","    X_train, y_train, X_test, y_test = data\n","\n","    # Make a list of param grids by combining the preprocessing and classifier hyperparameters\n","    param_grids = construct_param_grids_list(base_param_grid, key=\"classifier\")\n","\n","    # Iterate over each classifier\n","    tuning.run_model_search(\n","        X_train,\n","        y_train,\n","        X_test,\n","        y_test,\n","        param_grids,\n","        pipeline,\n","        task_type=\"classification\",\n","        refit_classification=\"f1-score\",\n","        output_dir=os.path.join(RUNS_DIR, \"final_classifiers\"),\n","        additional_save_on_run={\n","            \"categorical_encoder\": (categorical_encoder, \"pkl\"),\n","            \"categorical_cols\": (categorical_cols, \"pkl\"), \n","            \"numerical_cols\": (numerical_cols, \"pkl\"),\n","            \"correlated_cols\": (correlated_cols, \"txt\"),\n","            \"df_train_desc\": (df_train_desc, \"txt\"),\n","        },\n","        use_cuml=USE_CUML,\n","        subset=2,\n","        tabs=1,\n","        verbose=10,\n","    )"]},{"cell_type":"code","execution_count":null,"id":"001d77b0","metadata":{},"outputs":[],"source":["summary_df = tuning.plot_runs(\n","    os.path.join(RUNS_DIR, \"final_classifiers_mutual_info_classif\"),\n","    [\"f1-score\"],\n","    sort_by=\"f1-score\",\n","    runs_names_mapping={\n","        \"GradientBoostingClassifier\": \"GBC\",\n","        \"LinearDiscriminantAnalysis\": \"LDA\",\n","        \"QuadraticDiscriminantAnalysis\": \"QDA\",\n","        \"PassiveAggressiveClassifier\": \"PAC\",\n","    },\n","    summary_table=True,\n","    figsize=(6, 3),\n","    style=PAPER_STYLE,\n","    title=None,\n","    save_path=os.path.join(RUNS_DIR, \"final_classifiers_mutual_info_classif\", \"model_performance_comparison\"),\n","    save_format=\"pdf\",\n","    dpi=600,\n","    xtick_rotation=30,\n","    xtick_size=6,\n","    annotate_values=True,\n","    grid=True,\n","    use_adjust_text=True,\n","    y_max=1\n",")"]},{"cell_type":"markdown","id":"URLzKp2J2x6i","metadata":{"id":"URLzKp2J2x6i"},"source":["Now evaluate your model. Check the appendix for details on micro, macro and weighted averaging"]},{"cell_type":"code","execution_count":null,"id":"04624ba4","metadata":{},"outputs":[],"source":["best_run = \"run_20240823_215649_XGBClassifier_\"\n","best_run_path = os.path.join(RUNS_DIR, \"final_classifiers_mutual_info_classif\", best_run)\n","best_classifier = tuning.load_run_model(best_run_path)\n","best_classifier_name = best_run.split(\"_\")[-2]\n","categorical_encoder = tuning.load_from_run(best_run_path, \"categorical_encoder\")\n","categorical_cols = tuning.load_from_run(best_run_path, \"categorical_cols\")\n","X_train, y_train, X_test, y_test = tuning.load_from_run(best_run_path, \"data\")"]},{"cell_type":"code","execution_count":null,"id":"ff8e0e50","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":396,"status":"ok","timestamp":1716809077809,"user":{"displayName":"Jonas Klesen","userId":"16201315161566161330"},"user_tz":-120},"id":"ff8e0e50","outputId":"faa68cba-35cf-4203-c592-4879fdb63226"},"outputs":[],"source":["datasets = {\"training data\": [X_train, y_train], \"validation data\": [X_test, y_test]}\n","\n","for split_name, dataset in datasets.items():\n","    X_i, y_i = dataset\n","    y_pred = best_classifier.predict(X_i)\n","    print(f\"\\nSplit: {split_name}\")\n","\n","    print(skm.classification_report(y_i, y_pred))"]},{"cell_type":"code","execution_count":null,"id":"aa530634","metadata":{},"outputs":[],"source":["X_transformed = best_classifier[:-1].transform(X_train)\n","\n","if not NO_PLOTS:\n","    plots.visualize(\n","        X_transformed,\n","        n_components=3,\n","        method=\"pca\",\n","        indices=[y_train == 0, y_train == 1],\n","        labels=[\n","            f\"{CLASS_MAPPING[0]}, {len(y_train[y_train == 0])} samples\",\n","            f\"{CLASS_MAPPING[1]}, {len(y_train[y_train == 1])} samples\",\n","        ],\n","        style=PAPER_STYLE,\n","    )"]},{"cell_type":"code","execution_count":null,"id":"ba87f083","metadata":{},"outputs":[],"source":["plots.plot_pr_roc_curves(\n","    best_classifier,\n","    X_test,\n","    y_test,\n","    clf_name=best_classifier_name,\n","    style=PAPER_STYLE,\n",")"]},{"cell_type":"code","execution_count":null,"id":"dd9c1875","metadata":{},"outputs":[],"source":["print(\n","    f\"Original f1-score: {scorers.clf_scorers['f1_macro'](best_classifier, X_test, y_test)}\"\n",")\n","\n","tuned_model = TunedThresholdClassifierCV(\n","    estimator=best_classifier,\n","    scoring=scorers.clf_scorers[\"f1_macro\"],\n","    store_cv_results=True,\n",")\n","tuned_model.fit(X_train, y_train)\n","print(f\"{tuned_model.best_threshold_ = :0.2f}\")\n","print(\n","    f\"After tuning f1-score: {scorers.clf_scorers['f1_macro'](tuned_model, X_test, y_test)}\"\n",")"]},{"cell_type":"code","execution_count":null,"id":"72fc5f49","metadata":{},"outputs":[],"source":["# Comparison of the cut-off point for the vanilla and tuned model\n","plots.plot_pr_roc_curves(\n","    best_classifier,\n","    X_test,\n","    y_test,\n","    tuned_model,\n","    clf_name=best_classifier_name,\n","    show_objective_score=False,\n","    style=PAPER_STYLE,\n",")"]},{"cell_type":"code","execution_count":null,"id":"5c352d2a","metadata":{},"outputs":[],"source":["datasets = {\"training data\": [X_train, y_train], \"validation data\": [X_test, y_test]}\n","\n","for split_name, dataset in datasets.items():\n","    X_i, y_i = dataset\n","    y_pred = tuned_model.predict(X_i)\n","    print(f\"\\nSplit: {split_name}\")\n","\n","    print(skm.classification_report(y_i, y_pred))"]},{"cell_type":"markdown","id":"sGxdMSXO2-xH","metadata":{"id":"sGxdMSXO2-xH"},"source":["At this point, we can use our model to predict healthcare utilization on the test set.\n","\n","We again need to follow a specific namim format when saving the predictions. Similarly to before, the name of the file should be `<TEAM_ID>__<SPLIT>__clf_pred.npy`.\n","\n"]},{"cell_type":"code","execution_count":127,"id":"2b25b275","metadata":{},"outputs":[],"source":["best_classifier = tuned_model"]},{"cell_type":"code","execution_count":null,"id":"d0cea870","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716808592027,"user":{"displayName":"Jonas Klesen","userId":"16201315161566161330"},"user_tz":-120},"id":"d0cea870"},"outputs":[],"source":["# Run this to save a file with your predictions on the test set to be submitted\n","split = \"test_public\"  # replace by 'test_private' for FINAL submission\n","\n","df_test = pd.read_csv(f\"data/{split}.csv\")\n","\n","# Process data\n","# Make sure that we keep only the categorical cols that exist here\n","categorical_cols = helpers.remove_non_existent_columns(categorical_cols, df_test.columns)\n","\n","# Handle the categorical columns in the test set\n","df_test = helpers.encode_categorical_cols(\n","    df_test, categorical_cols, categorical_encoder, return_only_encoded=False\n",")\n","\n","y_hat = best_classifier.predict(df_test)\n","\n","# Save the results with the format <TEAM_ID>__<SPLIT>__clf_pred.npy\n","\n","folder = \"./results\"\n","np.save(os.path.join(folder, f\"{team_id}__{split}__clf_pred.npy\"), y_hat)"]},{"cell_type":"markdown","id":"CMofiwvW5Sd8","metadata":{"id":"CMofiwvW5Sd8"},"source":["# Submission to CMS"]},{"cell_type":"markdown","id":"n-7pww1o3iWN","metadata":{"id":"n-7pww1o3iWN"},"source":["Put your .npy files for both regression and classification tasks in the same zip file. Please name the file as `<TEAM_ID>.zip` (e.g. `123.zip`) and upload it to CMS system. It is essential that the files inside the .zip are named as follow:\n","\n","`<TEAM_ID>__<SPLIT>__reg_pred.npy` \\\n","`<TEAM_ID>__<SPLIT>__clf_pred.npy`\n","\n","Above, `<SPLIT>` should correspond to `test_public` for the leaderboard and `test_private` for the final submission.\n","As long as the `test_private.csv` data file is not released yet, the zip will contain only two files.\n"]},{"cell_type":"code","execution_count":null,"id":"B75zOG1F-KAQ","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1716808592027,"user":{"displayName":"Jonas Klesen","userId":"16201315161566161330"},"user_tz":-120},"id":"B75zOG1F-KAQ"},"outputs":[],"source":["# Run this to save a file with your predictions on the test set to be submitted\n","split = \"test_private\"  # replace by 'test_private' for FINAL submission\n","\n","df_test = pd.read_csv(f\"data/{split}.csv\")\n","\n","# Process data\n","# Make sure that we keep only the categorical cols that exist here\n","categorical_cols = helpers.remove_non_existent_columns(categorical_cols, df_test.columns)\n","\n","# Handle the categorical columns in the test set\n","df_test = helpers.encode_categorical_cols(\n","    df_test, categorical_cols, categorical_encoder, return_only_encoded=False\n",")\n","\n","y_hat = best_classifier.predict(df_test)\n","\n","# Save the results with the format <TEAM_ID>__<SPLIT>__clf_pred.npy\n","\n","folder = \"./results/test_private\"\n","helpers.ensure_directory_exists(folder)\n","np.save(os.path.join(folder, f\"{team_id}__{split}__clf_pred.npy\"), y_hat)"]},{"cell_type":"markdown","id":"340467b3","metadata":{"id":"340467b3"},"source":["### Appendix: Reminders about macro and micro averaging:\n","\n","When evaluating a classification model using `skm.classification_report(y_i, y_pred)` as done above, we get a macro and a weighted average.\n","\n","In the context of computing F1-score, \"macro\" and \"micro\" averaging are two commonly used techniques to aggregate the per-class F1-scores.\n","\n","**Micro-average**: Compute the F1-score globally by counting the total true positives, false negatives, and false positives over all classes, and then calculating precision, recall, and F1-score using these aggregated values.\n","\n","**Macro-average**: Calculate the F1-score for each class separately, and then take the average of these per-class F1-scores.\n","\n","The main difference between these two techniques is the way they treat class imbalance. Micro-average treats all classes equally, regardless of their size, while macro-average treats each class equally, regardless of the number of samples in that class.\n","\n","Micro-average is often used when we care about overall performance across all classes, and we want to give more weight to the performance on larger classes. In contrast, macro-average is often used when we want to evaluate the performance on each class separately and give equal weight to each class.\n","\n","\n","In addition to micro and macro averaging, there is another common technique for computing the F1-score called **weighted averaging**.\n","\n","**Weighted averaging** is similar to macro averaging in that it computes the per-class F1-score and then takes the average of these scores. However, unlike macro averaging, weighted averaging takes into account the number of samples in each class when computing the average. Specifically, the weighted average is computed as follows:\n","\n","- Compute the F1-score for each class separately.\n","- Compute the weight for each class as the number of samples in that class divided by the total number of samples.\n","- Compute the weighted average of the per-class F1-scores, where each per-class F1-score is weighted by the weight of that class.\n","\n","The weighted average is commonly used when the dataset is imbalanced, meaning that some classes have many more samples than others. In such cases, using the simple average (macro-average) would give too much weight to the smaller classes, while using micro-average would give too much weight to the larger classes. The weighted average strikes a balance between these two approaches by giving more weight to the classes with more samples while still taking into account the performance of all classes.\n"]},{"cell_type":"markdown","id":"a82d2dc3","metadata":{"id":"a82d2dc3"},"source":["When computing the F1 score for the leaderboard and the final challenge results, we will be using the macro averaging strategy."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}
